<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="../css/master.css">
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
  <title>TweetGAN Project | Connor Hainje</title>
</head>

<body>

<div class="nav black">
  <div class="navbrand">
    <a href="../index.html">Connor Hainje</a>
  </div>
  <div class="navpages">
    <a href="../index.html">Home</a>
    &emsp;
    <a class="active" href="../portfolio.html">Portfolio</a>
    &emsp;
    <a href="../resources/resume.pdf">Resume</a>
  </div>
</div>

<div class="container white" id="splash">
  <p><a href="../portfolio.html">Portfolio</a> &gt; <b>TweetGAN project</b></p>
  <h1><b style="color: #1DA1F2">Tweet</b>GAN project</h1>
  <hr>
  <p>
    Final deliverables:
    <a href="resources/tweetgan/poster.pdf">poster</a>,
    <a href="resources/tweetgan/paper.pdf">paper</a>.
    <br><br>
    This was my final project for COS 484: Natural Language Processing. My partner, Kathryn Leung, and I worked to use the SeqGAN model for a textual GAN with datasets mined from Twitter. The final project involved a report and a poster, both of which can be found above.
    <br><br>
    For this project, we mined tweets from two "genres" of Twitter accounts: news and university accounts. To do so, I registered the project with Twitter Developers to obtain API access tokens, and then wrote a script in Python using the Tweepy package to mine tweets. We decided on two lists of twelve accounts each; these lists are viewable in the paper. Then, as of January 8, I mined the most recent 200 tweets from each account.
    <br><br>
    These tweets were processed to remove URLs, and then shuffled and split into training and testing sets. Then, we used a PyTorch implementation of SeqGAN called TextGAN (<a href="https://github.com/williamSYSU/TextGAN-PyTorch">GitHub repo</a>). We trained this model first with the news tweets and then with the university tweets. Some samples generated by the GAN can be seen in the poster and in the paper.
    <br><br>
    When analyzing the metrics, however, we found that unfortunately the GAN did not benefit much from adversarial training; for both datasets, its performance basically stagnated after pretraining ended. We believe that the this is because the parameters used were nonoptimal, however, we did not have the time to conduct a broad enough parameter sweep to find a better set of parameters.
    <br><br>
    Even still, it was a fun project, and I'm especially proud of the poster and paper that we put together. Our results were less than groundbreaking, but I think that our final deliverables were quite high quality, and I found the whole experience worth sharing.
  </p>
</div>

<footer class="black">
  <p> &copy; 2020 Connor Hainje. </p>
</footer>

</body>
</html>
